{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=skyblue>示範 PyTorch 的淺度機器學習（典型的 Neural Network）應用在AT&T 人臉辨識的訓練與測試過程</font>**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<font color=blue>Load data and prepare for PyTorch Tensor</font>*\n",
    "\n",
    "Tensors are used in machine learning to represent data and model parameters. They are used to encode the inputs and outputs of a model as well as the model’s parameters. In PyTorch, tensors are used to represent the inputs and outputs of a model as well as the model’s parameters.\n",
    "\n",
    "Torch is a scientific computing framework with wide support for machine learning algorithms that puts GPUs first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(r'D:\\ys\\B4_ShallowML\\ShallowML\\ClassData\\face_data.csv')\n",
    "n_persons = df['target'].nunique() \n",
    "X = np.array(df.drop('target', axis=1)) # 400 x 4096\n",
    "y = np.array(df['target'])\n",
    "\n",
    "test_size = 0.3\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size) # deafult test_size=0.25\n",
    "\n",
    "# prepare data for PyTorch Tensor\n",
    "X_train = torch.from_numpy(X_train).float() # convert to float tensor\n",
    "y_train = torch.from_numpy(y_train).float() # \n",
    "train_dataset = TensorDataset(X_train, y_train) # create your datset\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_test = torch.from_numpy(y_test).float()\n",
    "test_dataset = TensorDataset(X_test, y_test) # create your datset\n",
    "\n",
    "# create dataloader for PyTorch\n",
    "batch_size = 32 # 32, 64, 128, 256\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) # convert to dataloader\n",
    "test_loader = DataLoader(test_dataset, batch_size=len(X_test), shuffle=False) # 不設定 batch_size, 會把所有資料放在一起"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.reshape(X_train, (X_train.shape[0], 1, 64, 64)) # 400 x 1 x 64 x 64 for CNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "*<font color=blue>Set up NN  model</font>*\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "MLP(\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=4096, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=40, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# select device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = \"cpu\" # run faster than cuda in some cases\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "# Create a neural network\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(64*64, 512), # image length 64x64=4096,  fully connected layer\n",
    "            nn.ReLU(), # you may want to take ReLU out to see what happen\n",
    "            nn.Linear(512, 128), # second hidden layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 40) # 40 classes,  fully connected layer\n",
    "            # nn.Softmax()\n",
    "        )\n",
    "    # Specify how data will pass through this model\n",
    "    def forward(self, x):\n",
    "        # out = self.mlp(x) \n",
    "\n",
    "        # Apply softmax to x here~\n",
    "        x = self.mlp(x) # pass through the model\n",
    "        out = F.log_softmax(x, dim=1) # it’s faster and has better numerical propertie than softmax\n",
    "        # out = F.softmax(x, dim=1)\n",
    "        return out\n",
    "\n",
    "# define model, optimizer, loss function\n",
    "model = MLP().to(device) # start an instance\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # default lreaning rate=1e-3\n",
    "loss_fun = nn.CrossEntropyLoss() # define loss function\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<font color=blue>Test the forward process of the nn model</font>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 40])\n",
      "torch.Size([1, 40])\n",
      "tensor([[0.0429, 0.0240, 0.0200, 0.0200, 0.0310, 0.0216, 0.0294, 0.0215, 0.0174,\n",
      "         0.0197, 0.0263, 0.0216, 0.0289, 0.0280, 0.0158, 0.0205, 0.0253, 0.0301,\n",
      "         0.0305, 0.0284, 0.0318, 0.0295, 0.0207, 0.0213, 0.0351, 0.0253, 0.0219,\n",
      "         0.0314, 0.0296, 0.0258, 0.0177, 0.0268, 0.0269, 0.0212, 0.0207, 0.0190,\n",
      "         0.0146, 0.0196, 0.0365, 0.0216]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0])\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 64 * 64) \n",
    "m1 = nn.Linear(64*64, 512)\n",
    "output = m1(input)\n",
    "print(output.size())\n",
    "output = F.relu(output)\n",
    "print(output.size())\n",
    "m2 = nn.Linear(512, 40)\n",
    "output = m2(output)\n",
    "print(output.size())\n",
    "# output = F.log_softmax(output, dim=1)\n",
    "output = F.softmax(output, dim=1)\n",
    "print(output.size())\n",
    "print(output)\n",
    "print(output.argmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([280, 512])\n",
      "torch.Size([280, 40])\n"
     ]
    }
   ],
   "source": [
    "output = nn.Linear(64*64, 512)(X_train)\n",
    "print(output.shape)\n",
    "pr = nn.ReLU()(output)\n",
    "p = nn.Linear(512, 40)(pr)\n",
    "print(p.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "*<font color=yellow>Training</font>*\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch 0 | Batch 0 | Loss   3.68\n",
      "\tEpoch 0 | Batch 4 | Loss   3.93\n",
      "\tEpoch 0 | Batch 8 | Loss   3.80\n",
      "Epoch 0 | Loss   3.81 | train accuracy 0.0143\n",
      "\tEpoch 1 | Batch 0 | Loss   3.68\n",
      "\tEpoch 1 | Batch 4 | Loss   3.79\n",
      "\tEpoch 1 | Batch 8 | Loss   3.66\n",
      "Epoch 1 | Loss   3.72 | train accuracy 0.0179\n",
      "\tEpoch 2 | Batch 0 | Loss   3.68\n",
      "\tEpoch 2 | Batch 4 | Loss   3.63\n",
      "\tEpoch 2 | Batch 8 | Loss   3.72\n",
      "Epoch 2 | Loss   3.65 | train accuracy 0.0357\n",
      "\tEpoch 3 | Batch 0 | Loss   3.61\n",
      "\tEpoch 3 | Batch 4 | Loss   3.62\n",
      "\tEpoch 3 | Batch 8 | Loss   3.64\n",
      "Epoch 3 | Loss   3.63 | train accuracy 0.0679\n",
      "\tEpoch 4 | Batch 0 | Loss   3.61\n",
      "\tEpoch 4 | Batch 4 | Loss   3.59\n",
      "\tEpoch 4 | Batch 8 | Loss   3.55\n",
      "Epoch 4 | Loss   3.59 | train accuracy 0.0286\n",
      "\tEpoch 5 | Batch 0 | Loss   3.51\n",
      "\tEpoch 5 | Batch 4 | Loss   3.58\n",
      "\tEpoch 5 | Batch 8 | Loss   3.54\n",
      "Epoch 5 | Loss   3.55 | train accuracy 0.0571\n",
      "\tEpoch 6 | Batch 0 | Loss   3.51\n",
      "\tEpoch 6 | Batch 4 | Loss   3.45\n",
      "\tEpoch 6 | Batch 8 | Loss   3.50\n",
      "Epoch 6 | Loss   3.51 | train accuracy 0.0857\n",
      "\tEpoch 7 | Batch 0 | Loss   3.45\n",
      "\tEpoch 7 | Batch 4 | Loss   3.41\n",
      "\tEpoch 7 | Batch 8 | Loss   3.41\n",
      "Epoch 7 | Loss   3.42 | train accuracy 0.0929\n",
      "\tEpoch 8 | Batch 0 | Loss   3.28\n",
      "\tEpoch 8 | Batch 4 | Loss   3.33\n",
      "\tEpoch 8 | Batch 8 | Loss   3.46\n",
      "Epoch 8 | Loss   3.35 | train accuracy 0.1286\n",
      "\tEpoch 9 | Batch 0 | Loss   3.31\n",
      "\tEpoch 9 | Batch 4 | Loss   3.15\n",
      "\tEpoch 9 | Batch 8 | Loss   3.05\n",
      "Epoch 9 | Loss   3.23 | train accuracy 0.2179\n",
      "\tEpoch 10 | Batch 0 | Loss   3.23\n",
      "\tEpoch 10 | Batch 4 | Loss   3.18\n",
      "\tEpoch 10 | Batch 8 | Loss   3.15\n",
      "Epoch 10 | Loss   3.14 | train accuracy 0.2071\n",
      "\tEpoch 11 | Batch 0 | Loss   2.90\n",
      "\tEpoch 11 | Batch 4 | Loss   3.06\n",
      "\tEpoch 11 | Batch 8 | Loss   2.92\n",
      "Epoch 11 | Loss   2.99 | train accuracy 0.2214\n",
      "\tEpoch 12 | Batch 0 | Loss   2.89\n",
      "\tEpoch 12 | Batch 4 | Loss   2.80\n",
      "\tEpoch 12 | Batch 8 | Loss   2.81\n",
      "Epoch 12 | Loss   2.89 | train accuracy 0.1929\n",
      "\tEpoch 13 | Batch 0 | Loss   2.71\n",
      "\tEpoch 13 | Batch 4 | Loss   2.75\n",
      "\tEpoch 13 | Batch 8 | Loss   2.90\n",
      "Epoch 13 | Loss   2.72 | train accuracy 0.2750\n",
      "\tEpoch 14 | Batch 0 | Loss   2.48\n",
      "\tEpoch 14 | Batch 4 | Loss   2.68\n",
      "\tEpoch 14 | Batch 8 | Loss   2.52\n",
      "Epoch 14 | Loss   2.58 | train accuracy 0.3071\n",
      "\tEpoch 15 | Batch 0 | Loss   2.39\n",
      "\tEpoch 15 | Batch 4 | Loss   2.40\n",
      "\tEpoch 15 | Batch 8 | Loss   2.45\n",
      "Epoch 15 | Loss   2.44 | train accuracy 0.3143\n",
      "\tEpoch 16 | Batch 0 | Loss   2.37\n",
      "\tEpoch 16 | Batch 4 | Loss   2.16\n",
      "\tEpoch 16 | Batch 8 | Loss   2.01\n",
      "Epoch 16 | Loss   2.32 | train accuracy 0.3643\n",
      "\tEpoch 17 | Batch 0 | Loss   2.33\n",
      "\tEpoch 17 | Batch 4 | Loss   2.14\n",
      "\tEpoch 17 | Batch 8 | Loss   2.25\n",
      "Epoch 17 | Loss   2.20 | train accuracy 0.4143\n",
      "\tEpoch 18 | Batch 0 | Loss   1.70\n",
      "\tEpoch 18 | Batch 4 | Loss   1.90\n",
      "\tEpoch 18 | Batch 8 | Loss   2.09\n",
      "Epoch 18 | Loss   2.04 | train accuracy 0.4821\n",
      "\tEpoch 19 | Batch 0 | Loss   2.09\n",
      "\tEpoch 19 | Batch 4 | Loss   1.90\n",
      "\tEpoch 19 | Batch 8 | Loss   2.07\n",
      "Epoch 19 | Loss   1.92 | train accuracy 0.5071\n",
      "\tEpoch 20 | Batch 0 | Loss   1.83\n",
      "\tEpoch 20 | Batch 4 | Loss   1.81\n",
      "\tEpoch 20 | Batch 8 | Loss   1.94\n",
      "Epoch 20 | Loss   1.82 | train accuracy 0.5500\n",
      "\tEpoch 21 | Batch 0 | Loss   1.92\n",
      "\tEpoch 21 | Batch 4 | Loss   1.53\n",
      "\tEpoch 21 | Batch 8 | Loss   1.78\n",
      "Epoch 21 | Loss   1.76 | train accuracy 0.4893\n",
      "\tEpoch 22 | Batch 0 | Loss   1.59\n",
      "\tEpoch 22 | Batch 4 | Loss   1.68\n",
      "\tEpoch 22 | Batch 8 | Loss   1.88\n",
      "Epoch 22 | Loss   1.70 | train accuracy 0.5679\n",
      "\tEpoch 23 | Batch 0 | Loss   1.68\n",
      "\tEpoch 23 | Batch 4 | Loss   1.86\n",
      "\tEpoch 23 | Batch 8 | Loss   1.75\n",
      "Epoch 23 | Loss   1.68 | train accuracy 0.4857\n",
      "\tEpoch 24 | Batch 0 | Loss   1.44\n",
      "\tEpoch 24 | Batch 4 | Loss   1.61\n",
      "\tEpoch 24 | Batch 8 | Loss   1.23\n",
      "Epoch 24 | Loss   1.56 | train accuracy 0.5964\n",
      "\tEpoch 25 | Batch 0 | Loss   1.29\n",
      "\tEpoch 25 | Batch 4 | Loss   1.81\n",
      "\tEpoch 25 | Batch 8 | Loss   1.24\n",
      "Epoch 25 | Loss   1.39 | train accuracy 0.6964\n",
      "\tEpoch 26 | Batch 0 | Loss   0.99\n",
      "\tEpoch 26 | Batch 4 | Loss   1.27\n",
      "\tEpoch 26 | Batch 8 | Loss   1.62\n",
      "Epoch 26 | Loss   1.32 | train accuracy 0.6536\n",
      "\tEpoch 27 | Batch 0 | Loss   1.27\n",
      "\tEpoch 27 | Batch 4 | Loss   1.08\n",
      "\tEpoch 27 | Batch 8 | Loss   1.41\n",
      "Epoch 27 | Loss   1.28 | train accuracy 0.6607\n",
      "\tEpoch 28 | Batch 0 | Loss   1.08\n",
      "\tEpoch 28 | Batch 4 | Loss   1.19\n",
      "\tEpoch 28 | Batch 8 | Loss   1.32\n",
      "Epoch 28 | Loss   1.22 | train accuracy 0.7179\n",
      "\tEpoch 29 | Batch 0 | Loss   0.95\n",
      "\tEpoch 29 | Batch 4 | Loss   1.19\n",
      "\tEpoch 29 | Batch 8 | Loss   1.28\n",
      "Epoch 29 | Loss   1.19 | train accuracy 0.6786\n",
      "\tEpoch 30 | Batch 0 | Loss   1.10\n",
      "\tEpoch 30 | Batch 4 | Loss   1.10\n",
      "\tEpoch 30 | Batch 8 | Loss   1.22\n",
      "Epoch 30 | Loss   1.15 | train accuracy 0.6893\n",
      "\tEpoch 31 | Batch 0 | Loss   0.99\n",
      "\tEpoch 31 | Batch 4 | Loss   1.05\n",
      "\tEpoch 31 | Batch 8 | Loss   1.20\n",
      "Epoch 31 | Loss   1.08 | train accuracy 0.7250\n",
      "\tEpoch 32 | Batch 0 | Loss   0.89\n",
      "\tEpoch 32 | Batch 4 | Loss   0.97\n",
      "\tEpoch 32 | Batch 8 | Loss   0.88\n",
      "Epoch 32 | Loss   1.03 | train accuracy 0.7643\n",
      "\tEpoch 33 | Batch 0 | Loss   0.80\n",
      "\tEpoch 33 | Batch 4 | Loss   1.08\n",
      "\tEpoch 33 | Batch 8 | Loss   1.44\n",
      "Epoch 33 | Loss   1.09 | train accuracy 0.7143\n",
      "\tEpoch 34 | Batch 0 | Loss   0.90\n",
      "\tEpoch 34 | Batch 4 | Loss   1.00\n",
      "\tEpoch 34 | Batch 8 | Loss   0.86\n",
      "Epoch 34 | Loss   0.98 | train accuracy 0.7500\n",
      "\tEpoch 35 | Batch 0 | Loss   0.75\n",
      "\tEpoch 35 | Batch 4 | Loss   0.82\n",
      "\tEpoch 35 | Batch 8 | Loss   1.14\n",
      "Epoch 35 | Loss   0.88 | train accuracy 0.7679\n",
      "\tEpoch 36 | Batch 0 | Loss   0.85\n",
      "\tEpoch 36 | Batch 4 | Loss   0.79\n",
      "\tEpoch 36 | Batch 8 | Loss   0.72\n",
      "Epoch 36 | Loss   0.79 | train accuracy 0.8357\n",
      "\tEpoch 37 | Batch 0 | Loss   0.72\n",
      "\tEpoch 37 | Batch 4 | Loss   0.78\n",
      "\tEpoch 37 | Batch 8 | Loss   0.72\n",
      "Epoch 37 | Loss   0.75 | train accuracy 0.8500\n",
      "\tEpoch 38 | Batch 0 | Loss   0.68\n",
      "\tEpoch 38 | Batch 4 | Loss   0.52\n",
      "\tEpoch 38 | Batch 8 | Loss   0.81\n",
      "Epoch 38 | Loss   0.71 | train accuracy 0.8464\n",
      "\tEpoch 39 | Batch 0 | Loss   0.73\n",
      "\tEpoch 39 | Batch 4 | Loss   0.54\n",
      "\tEpoch 39 | Batch 8 | Loss   0.77\n",
      "Epoch 39 | Loss   0.69 | train accuracy 0.8500\n",
      "\tEpoch 40 | Batch 0 | Loss   0.55\n",
      "\tEpoch 40 | Batch 4 | Loss   0.63\n",
      "\tEpoch 40 | Batch 8 | Loss   0.73\n",
      "Epoch 40 | Loss   0.70 | train accuracy 0.8214\n",
      "\tEpoch 41 | Batch 0 | Loss   0.43\n",
      "\tEpoch 41 | Batch 4 | Loss   0.58\n",
      "\tEpoch 41 | Batch 8 | Loss   0.68\n",
      "Epoch 41 | Loss   0.67 | train accuracy 0.8607\n",
      "\tEpoch 42 | Batch 0 | Loss   0.49\n",
      "\tEpoch 42 | Batch 4 | Loss   0.53\n",
      "\tEpoch 42 | Batch 8 | Loss   0.71\n",
      "Epoch 42 | Loss   0.60 | train accuracy 0.8821\n",
      "\tEpoch 43 | Batch 0 | Loss   0.58\n",
      "\tEpoch 43 | Batch 4 | Loss   0.55\n",
      "\tEpoch 43 | Batch 8 | Loss   0.58\n",
      "Epoch 43 | Loss   0.55 | train accuracy 0.9071\n",
      "\tEpoch 44 | Batch 0 | Loss   0.46\n",
      "\tEpoch 44 | Batch 4 | Loss   0.67\n",
      "\tEpoch 44 | Batch 8 | Loss   0.61\n",
      "Epoch 44 | Loss   0.59 | train accuracy 0.8679\n",
      "\tEpoch 45 | Batch 0 | Loss   0.46\n",
      "\tEpoch 45 | Batch 4 | Loss   0.55\n",
      "\tEpoch 45 | Batch 8 | Loss   0.62\n",
      "Epoch 45 | Loss   0.57 | train accuracy 0.8929\n",
      "\tEpoch 46 | Batch 0 | Loss   0.35\n",
      "\tEpoch 46 | Batch 4 | Loss   0.56\n",
      "\tEpoch 46 | Batch 8 | Loss   0.40\n",
      "Epoch 46 | Loss   0.51 | train accuracy 0.8893\n",
      "\tEpoch 47 | Batch 0 | Loss   0.52\n",
      "\tEpoch 47 | Batch 4 | Loss   0.43\n",
      "\tEpoch 47 | Batch 8 | Loss   0.61\n",
      "Epoch 47 | Loss   0.47 | train accuracy 0.9214\n",
      "\tEpoch 48 | Batch 0 | Loss   0.32\n",
      "\tEpoch 48 | Batch 4 | Loss   0.81\n",
      "\tEpoch 48 | Batch 8 | Loss   0.60\n",
      "Epoch 48 | Loss   0.45 | train accuracy 0.9214\n",
      "\tEpoch 49 | Batch 0 | Loss   0.48\n",
      "\tEpoch 49 | Batch 4 | Loss   0.36\n",
      "\tEpoch 49 | Batch 8 | Loss   0.29\n",
      "Epoch 49 | Loss   0.41 | train accuracy 0.9250\n",
      "Finished ... Loss  0.4101 | train accuracy 0.9250\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "epochs = 50 # Repeat the whole dataset epochs times\n",
    "model.train() # Sets the module in training mode. The training model allow the parameters to be updated during backpropagation.\n",
    "for epoch in range(epochs):\n",
    "# for epoch in tqdm(range(epochs)):\n",
    "    trainAcc = 0\n",
    "    samples = 0\n",
    "    losses = []\n",
    "    for batch_num, input_data in enumerate(train_loader):\n",
    "    # for batch_num, input_data in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "        \n",
    "        x, y = input_data\n",
    "        x = x.to(device).float()\n",
    "        y = y.to(device)\n",
    "\n",
    "        # perform training based on the backpropagation\n",
    "        y_pre = model(x) # predict y\n",
    "        loss = loss_fun(y_pre, y.long()) # the loss function nn.CrossEntropyLoss()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad() # Zeros the gradients accumulated from the previous batch/step of the model\n",
    "        loss.backward() # Performs backpropagation and calculates the gradients\n",
    "        optimizer.step() # Updates the weights in our neural network based on the results of backpropagation\n",
    "        \n",
    "        # Record the training accuracy for each batch\n",
    "        trainAcc += (y_pre.argmax(dim=1) == y).sum().item() # comparison\n",
    "        samples += y.size(0)\n",
    "        if batch_num % 4 == 0:\n",
    "            print('\\tEpoch %d | Batch %d | Loss %6.2f' % (epoch, batch_num, loss.item()))\n",
    "    print('Epoch %d | Loss %6.2f | train accuracy %.4f' % (epoch, sum(losses)/len(losses), trainAcc/samples))\n",
    "\n",
    "print('Finished ... Loss %7.4f | train accuracy %.4f' % (sum(losses)/len(losses), trainAcc/samples))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<font color=blue>Testing (1)</font>*\n",
    "\n",
    "Compute test accuracy by batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:0.767\n"
     ]
    }
   ],
   "source": [
    "model.eval() \n",
    "testAcc = 0\n",
    "samples = 0\n",
    "with torch.no_grad():\n",
    "    for x, y_truth in test_loader:\n",
    "        x = x.to(device).float()\n",
    "        y_truth = y_truth.to(device)\n",
    "        y_pre = model(x).argmax(dim=1) # the predictions for the batch\n",
    "        testAcc += (y_pre == y_truth).sum().item() # comparison\n",
    "        samples += y_truth.size(0)\n",
    "\n",
    "    print('Test Accuracy:{:.3f}'.format(testAcc/samples))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<font color=blue>Testing (2)</font>*\n",
    "\n",
    "Compute the test accuracy and record the result for each test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:0.767\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# use eval() in conjunction with a torch.no_grad() context, \n",
    "# meaning that gradient computation is turned off in evaluation mode\n",
    "model.eval() \n",
    "testAcc = 0\n",
    "samples = 0\n",
    "\n",
    "with open('mlp_att.csv', 'w') as f:\n",
    "    fieldnames = ['ImageId', 'Label', 'Ground_Truth']\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames, lineterminator = '\\n')\n",
    "    writer.writeheader()\n",
    "    image_id = 1\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y_truth in test_loader:\n",
    "            x = x.to(device).float()\n",
    "            y_truth = y_truth.to(device).long()\n",
    "            yIdx = 0\n",
    "            y_pre = model(x).argmax(dim=1) # the predictions for the batch\n",
    "            testAcc += (y_pre == y_truth).sum().item() # comparison\n",
    "            samples += y_truth.size(0)\n",
    "            for y in y_pre:\n",
    "                writer.writerow({fieldnames[0]: image_id,fieldnames[1]: y.item(), fieldnames[2]: y_truth[yIdx].item()})\n",
    "                image_id += 1\n",
    "                yIdx += 1\n",
    "\n",
    "        print('Test Accuracy:{:.3f}'.format(testAcc/samples))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ShallowML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
